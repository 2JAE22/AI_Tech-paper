"# AI_Tech-paper" 


# 1. Introduction
NaverAIboostCamp에서 소개한 논문들을 주제별로 정리한 폴더입니다.
현재는 CV(Computer Vision) 트랙의 논문을 중심으로 정리하고 있으며, 추후 모든 트랙으로 확장할 예정입니다.

# 2. CV 트랙 정리(Only Paper)
## 2.1 CV 이론
- [VGGNet](https://arxiv.org/abs/1409.1556)  
- [ResNet](https://arxiv.org/abs/1512.03385)  
- [ViT](https://arxiv.org/abs/2010.11929)
- [Grad-CAM](https://arxiv.org/abs/1610.02391)
- [mixup](https://arxiv.org/abs/1710.09412)
- [CutMix](https://arxiv.org/abs/1905.04899)
- [Fully Convolutional Networks for Semantic Segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)
- [SAM](https://arxiv.org/pdf/2304.02643)
- [DETR](https://arxiv.org/pdf/2005.12872)
- [Real-World Single Image Super-Resolution: A New Benchmark and A New Model](https://arxiv.org/abs/1904.00523)
- [Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700188.pdf)
- [Blind Super-Resolution Kernel Estimation using an Internal-GAN](https://arxiv.org/abs/1909.06581)
- [SpatialTracker: Tracking Any 2D Pixels in 3D Space](https://arxiv.org/abs/2404.04319)
- [CLIP huggingface implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py)
- [ImageBIND official implementation](https://github.com/facebookresearch/ImageBind)
- [LanguageBIND](https://arxiv.org/abs/2310.01852)
- [Flamingo pytorch implementation](https://github.com/lucidrains/flamingo-pytorch/blob/main/flamingo_pytorch/flamingo_pytorch.py)
- [LLaVA](https://llava-vl.github.io/)
- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)
- [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
- [DDPM](https://arxiv.org/abs/2006.11239)
- [LDM (Stable Diffusion)](https://arxiv.org/abs/2112.10752)
- [DDIM](https://arxiv.org/abs/2010.02502) 
- [3D MACHINE LEARNING](https://www.antoinetlc.com/blog-summary/3d-data-representations)
- [Mesh R-CNN](https://arxiv.org/abs/1906.02739)
- [NeRF](https://arxiv.org/abs/2003.08934)
- [3DGS](https://arxiv.org/abs/2308.04079)
- [DreamFusion](https://arxiv.org/abs/2209.14988)
- [Loper et al., SMPL: A Skinned Multi-Person Linear Model: SIGGRAPH 2015.](https://dl.acm.org/doi/10.1145/2816795.2818013)
- [Bogo et al., Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image: ECCV 2016.](https://arxiv.org/abs/1607.08128)
- [Anguelov et al., SCAPE: Shape Completion and Animation of People: SIGGRAPH 2005.](https://dl.acm.org/doi/10.1145/1073204.1073207)


## 2.2 CV 기초 프로젝트
- [A survey on Image Data Augmentation for Deep Learning](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)
- [AutoAugment: Learning Augmentation Strategies from Data](https://arxiv.org/abs/1805.09501)
- [RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/abs/1909.13719)
- [Fine-Grained Image Analysis with Deep Learning: A Survey](https://arxiv.org/abs/2111.06119)
- [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [CoAtNet: Marrying Convolution and Attention for All Data Sizes](https://arxiv.org/abs/2106.04803)
- [ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)
- [Multimodal Learning with Transformers: A Survey](https://arxiv.org/abs/2206.06488)
- [Self-supervised Learning: Generative or Contrastive](https://arxiv.org/abs/2006.08218)
- [Ensemble deep learning: A review](https://arxiv.org/abs/2104.02395)
  
## 2.3 Object Detection
- [R-CNN](https://arxiv.org/abs/1311.2524)
- [Fast R-CNN](https://arxiv.org/abs/1504.08083)
- [Faster R-CNN](https://arxiv.org/abs/1506.01497)
- [SPPNet](https://arxiv.org/abs/1406.4729)
- [FPN](https://arxiv.org/abs/1612.03144)
- [PAFPN](https://arxiv.org/abs/1803.01534)
- [DetectoRS](https://arxiv.org/abs/2006.02334)
- [EfficientDet (BiFPN)](https://arxiv.org/abs/1911.09070v7)
- [NasFPN](https://arxiv.org/abs/1904.07392)
- [AugFPN](https://arxiv.org/abs/1912.05384)
- [YOLO survey](https://arxiv.org/abs/2304.00501)
- [Retinanet (focal loss)](https://arxiv.org/abs/1708.02002)
- [SSD](https://arxiv.org/abs/1512.02325)
- [EfficientNet](https://arxiv.org/abs/1905.11946)
- [EfficientDet](https://arxiv.org/abs/1911.09070)
- [DCN](https://arxiv.org/abs/1703.06211)
- [DETR](https://arxiv.org/abs/2005.12872)
- [Swin](https://arxiv.org/abs/2103.14030)
- [YOLO v4](https://arxiv.org/abs/2004.10934)
- [M2Det](https://arxiv.org/abs/1811.04533)
- [CornerNet](https://arxiv.org/abs/1808.01244)

## 2.4 Data-Centric AI
- [DMLR](https://openreview.net/pdf?id=2kpu78QdeE)
- [Convolutional Character Networks](https://openaccess.thecvf.com/content_ICCV_2019/papers/Xing_Convolutional_Character_Networks_ICCV_2019_paper.pdf)
- [EAST](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf)
- [Data and its (dis)contents: A survey of dataset development and use in machine learning research](https://www.sciencedirect.com/science/article/pii/S2666389921001847)
- [Human-In-The-Loop에 대한 survey 논문](https://www.sciencedirect.com/science/article/abs/pii/S0167739X22001790?casa_token=5poWCKizHjIAAAAA:Z8eK3GMWCCwOncUmdz2J8JHGNYAx3N4MW_31Uq3CnWVQN2C6RXXtOqc50GveYglcudc9TiwhYKk)
- [다양한 task에 적용 가능한 IAA에 관한 논문](https://dl.acm.org/doi/10.1145/3485447.3512242)
- [LLM을 활용한 data annotation에 관한 survey 논문](https://arxiv.org/abs/2402.13446)
- [A survey on Image Data Augmentation for Deep Learning](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)
- [A survey of synthetic data augmentation methods in computer vision](https://arxiv.org/abs/2403.10075)



## 2.5 Semantic Segmentation

# 3. NLP 트랙 정리(Only Paper)
## 3.1 NLP 이론
## 3.2 NLP 기초 프로젝트
## 3.3 MRC
## 3.4 Data-Centric NLP
## 3.5 Generative for NLP


# 4. Recsys 트랙 정리(Only Paper)
## 4.1 Recsys 이론
## 4.2 ML 기초 프로젝트
## 4.3 Competitive DS
## 4.4 RecSys 기초프로젝트
## 4.5 Movie Rec



# 5. 공통 강의에서 소개한 논문 정리
## 5.1 Generative AI
## 5.2 Product Serving
## 5.3 최적화/경량화

# 6. Further Reading에 있었던 것들.
## 6.1 공통코스
### 6.1.1 Pytorch
- [Introduction to PyTorch — PyTorch Tutorials documentation](https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html)
- [텐서(Tensor) — 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)](https://tutorials.pytorch.kr/beginner/introyt/introyt1_tutorial.html)
- [torch.Tensor — PyTorch documentation](https://pytorch.org/docs/main/tensors.html)
- [부동소수점 - 백과사전](https://ko.wikipedia.org/wiki/%EB%B6%80%EB%8F%99%EC%86%8C%EC%88%98%EC%A0%90)
- [torch.randn — PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn)
- [torch.Tensor — PyTorch documentation](https://pytorch.org/docs/main/tensors.html)
- [GPU와 AI - 네이버 지식백과](https://terms.naver.com/entry.naver?docId=2080143&cid=50305&categoryId=50305)
- [torch.Tensor.view — PyTorch main documentation](https://pytorch.org/docs/main/generated/torch.Tensor.view.html#torch-tensor-view)
- [torch.reshape — PyTorch main documentation](https://pytorch.org/docs/main/generated/torch.reshape.html#torch-reshape)
- [Difference between view, reshape, transpose and permute in PyTorch](https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/)
- [torch.squeeze — PyTorch main documentation](https://pytorch.org/docs/main/generated/torch.squeeze.html#torch-squeeze)
- [Tensor 모양 설명](https://velog.io/@jk01019/broadcastto-repeat-repeatinterleave-view-reshape-expand-expandas-tile-flatten-unsqueeze-squeeze-stack-cat-d1n8ersb)
- [$L_p$ norm](https://en.m.wikipedia.org/wiki/Lp_space)
- [선형회귀](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)
- [경사하강법 학습방법](https://www.youtube.com/watch?v=IHZwWFHWa-w)
- [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)
- [PyTorch DataLoader — PyTorch main documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
- [BCELoss — PyTorch main documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)
- [BCEWithLogitsLoss — PyTorch main documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)
- [CrossEntropyLoss — PyTorch main documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)

### 6.1.2 ML LifeCycle
- [A survey on Image Data Augmentation for Deep Learning ](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0) 
- [Clipping](https://arxiv.org/pdf/1211.5063) 
- [LSTM — PyTorch main documentation ](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) 
- [Attention Is All You Need ](https://arxiv.org/pdf/1706.03762) 
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ](https://arxiv.org/pdf/1810.04805) 
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ](https://arxiv.org/pdf/2010.11929) 


### 6.1.3 EDA & DataViz
- [[Harvard Business Review] Boost Your Team’s Data Literacy](https://hbr.org/2020/02/boost-your-teams-data-literacy)
- [[Sequoia Capital] Why Data Science Matters](https://medium.com/sequoia-capital/why-data-science-matters-ee583f785a55)
- [[Sequoia Capital] Role of Data Scientist](https://medium.com/sequoia-capital/five-core-skills-of-a-data-scientist-fc044014fafa)
- [데이터 시각화 교과서 (클라우스 윌케 저)](https://clauswilke.com/dataviz/index.html)
- [Data Viz Project](https://datavizproject.com/)
- [Misleading Data Visualization](https://pressbooks.library.torontomu.ca/criticaldataliteracy/chapter/misleading-data-visualizations/)
- [[The Economists] Mistakes, we’ve drawn a few](https://medium.economist.com/mistakes-weve-drawn-a-few-8cdd8a42d368)
- [[Google ML Course] Types of Bias](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias?hl=ko)
- [[Github] Awesome Feature Engineering](https://github.com/aikho/awesome-feature-engineering)
- [[서울대 AI 연구원] 다차원 데이터 시각화와 AI (컴퓨터공학부 서진욱 교수)](https://www.youtube.com/watch?v=ymA1spEAd7M)
- [[Distill] How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)
- [Forecasting: Principles & Practice](https://otexts.com/fppkr/arima.html)
- [Hugging Face - NLP Course](https://huggingface.co/learn/nlp-course/ko/chapter1/2?fw=pt)
- [Text Visualization Browser](https://textvis.lnu.se/)
- [[Github] eugeneyan/applied-ml](https://github.com/eugeneyan/applied-ml)
- [[Material Design] Data Visualization](https://m2.material.io/design/communication/data-visualization.html)
- [잘못 사용된 시각화 모음 WTF.viz](https://viz.wtf/)
- [Startup Metrics for Pirates: AARRR! - Dave McClure](https://www.youtube.com/watch?v=irjgfW0BIrw)
- [데이터 시각화, 인지과학을 만나다 ](https://www.yes24.com/Product/Goods/19013968)
- [도널드 노만의 UX 디자인 특강 ](https://www.yes24.com/Product/Goods/59673763)
- [UX/UI의 10가지 심리학 법칙 ](https://product.kyobobook.co.kr/detail/S000212939982)

### 6.1.4 AI 개발 기초
- ["Clean Code: A Handbook of Agile Software Craftsmanship" by Robert C. Martin:](https://www.oreilly.com/library/view/clean-code-a/9780136083238/)
- ["Design Patterns: Elements of Reusable Object-Oriented Software" by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissidesn](https://www.oreilly.com/library/view/design-patterns-elements/0201633612/)
- ["모던 리눅스 교과서" by 마이클 하우센블라스](https://product.kyobobook.co.kr/detail/S000210138053)
- ["The Linux Command Line" by William Shotts:](https://wiki.lib.sun.ac.za/images/c/ca/TLCL-13.07.pdf)
- [Streamlit 공식 문서](https://docs.streamlit.io/)
- [Python 3.x Docs: Virtual Environments and Packages](https://docs.python.org/3/tutorial/venv.html)



## 6.2 CV
### 6.2.1 CV 이론   
- [한겨례 뉴스 “빌게이츠도 감탄한 최예진 교수, 생성형 AI 학습 데이터 공개해야"](https://www.hani.co.kr/arti/economy/economy_general/1128825.html)




### 6.2.2 CV 기초 프로젝트
- [Kaggle Competition](https://www.kaggle.com/competitions)
- [Image file format](https://en.wikipedia.org/wiki/Image_file_format)
- [Multilabel Image Classification Using Deep Learning](https://www.mathworks.com/help/deeplearning/ug/multilabel-image-classification-using-deep-learning.html)
- [Training with Pytorch](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)
- [Pytorch: Automatic Mixed Precision ](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)
- [Nvidia: Mixed-Precision-Training](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)
- [Tensorboard](https://www.tensorflow.org/tensorboard/get_started?hl=ko)
- [WandB](https://wandb.ai/site/ko/)  

### 6.2.3 Object Detection
- [mmdetection github](https://github.com/open-mmlab/mmdetection)
- [detectron2 github](https://github.com/facebookresearch/detectron2)
- [Paperswithcode Object Detection](https://paperswithcode.com/task/object-detection)
- [Kaggle](https://www.kaggle.com/competitions)

### 6.2.4 Data-Centric AI
- [AI 최신 활용 사례](https://www.content.upstage.ai/blog/insight/examples-of-artificial-intelligence)
- [현실 세계에서의 데이터중심 AI](https://ko.upstage.ai/blog/tech/data-centric-ai-in-the-real-world)
- [SynthText in the Wild Dataset](https://www.robots.ox.ac.uk/~vgg/data/scenetext/)
- [Tesseract (Off-the-shelf OCR open source)](https://tesseract-ocr.github.io/)
- [Data annotation에 대한 OpenCV의 blogpost](https://opencv.org/blog/data-annotation/)
- []()
- []()
- []()  
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()

### 6.2.5 Semantic Segmentation
- []()
- []()
- []()
- []()
- []()
- []()
- []()  
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()
- []()

## 6.3 NLP
### 6.3.1 NLP 이론
### 6.3.2 NLP 기초 프로젝트

### 6.3.3 MRC
### 6.3.4 Data-Centric NLP
### 6.3.5 Generative for NLP

## 6.4 Recsys
### 6.4.1 Recsys 이론
### 6.4.2 ML 기초 프로젝트
### 6.4.3 Competitive DS
### 6.4.4 RecSys 기초프로젝트
### 6.4.5 Movie Rec

# 7. Contribution 하는 방법

## 7.1 Fork를 한다.
- 원본 저장소를 내 계정으로 복사(Fork)

## 7.2 PR를 보낸다.
- Fork한 저장소에서 변경 내용을 작업한 뒤 Pull Request(Pull Request)를 생성

## 7.3 Approve를 받으면 Merge!
- 리뷰 후 승인을 받으면 원본 저장소에 변경사항이 병합(Merge)

>이 문서는 계속 업데이트 예정입니다.
궁금한 사항이나 제안이 있다면 Issue 혹은 Pull Request로 알려주세요!
감사합니다.